---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 18, 2025 @ 11:59PM"
author: "Jiaye Tian and UID: 306541095"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)
We will now derive the probability that a given observation is part of 
a bootstrap sample. 
Suppose that we obtain a bootstrap sample from a set of n observations.

**(a)**  
$1 - 1/n$  
The probability that the ùëóth observation is not the first bootstrap 
observation is 1 - 1/n.
Since there are n observations, each has an equal probability of 1/n of being 
chosen first. Therefore, the probability that the jth observation is not 
selected as the first bootstrap observation is 1 ‚àí 1/n.

**(b)**  
$1 - 1/n$  
Since each bootstrap observation is independently drawn with replacement, 
every selection follows the same probability distribution.

**(c)**
In bootstrapping, each sample is drawn independently with replacement from the 
original dataset of size n. The probability that the jth observation 
is not chosen in a single draw is 1‚àí1/n. Since we draw n times independently, 
the probability that it never appears in the bootstrap sample is 
$$
(1 - \frac{1}{n}) \cdots (1 - \frac{1}{n}) = (1 - \frac{1}{n})^n
$$

**(d)**  
When n = 5, the probability that the jth observation appears in the bootstrap 
sample is $P(\text{jth obs in bootstrap sample when n=5}) = 1 - (1 - \frac{1}{5})^5 = 0.672.$

**(e)**  
When n = 100, the probability that the jth observation appears in the bootstrap 
sample is $P(\text{jth obs in bootstrap sample when n=100}) = 1 - (1 - 1/100)^{100} = 0.634.$

**(f)**
When n = 1000, the probability that the jth observation appears in the bootstrap 
sample is $P(\text{jth obs in bootstrap sample when n=1000}) = 1 - (1 - \frac{1}{1000})^{1000} = 0.632.$

**(g)**
```{r}
n <- seq(1, 100000, by=10)

prob <- 1 - (1 - 1/n)^n

asymptote <- 1 - 1/exp(1)

asymptote

plot(n, prob, type="l", log="x", 
     xlab="n (log scale)", ylab="Probability of inclusion",
     main="Probability of an Observation in a Bootstrap Sample")

abline(h=1 - 1/exp(1), col="red", lty=2)
```
**(h)**
```{r}
contains_j <- rep(NA, 10000)
for (i in 1:10000) {
    contains_j[i] <- sum(sample(1:100, rep = TRUE) == 4) > 0
}
mean(contains_j)
```
From calculus, we know:

$$
\lim_{n\rightarrow\infty}\left(1 - \frac{1}{n}\right)^n = \frac{1}{e}.
$$

Thus, the probability that a bootstrap sample of size \( n \) contains the \( j \)th observation is:

$$
P(\text{included}) = 1 - \left(1 - \frac{1}{n}\right)^n.
$$

Taking the limit as \( n \to \infty \):

$$
\lim_{n \to \infty} P(\text{included}) = 1 - \frac{1}{e} \approx 0.632.
$$

So for large \( n \), each observation appears in the bootstrap sample about **63.2%** of the time.

## ISL Exercise 5.4.9 (20pts)
```{r}
library(MASS)
library(boot)
attach(Boston)
```

```{r}
mu.hat <- mean(medv)
mu.hat
```
**(b)**
```{r}
se.hat <- sd(medv) / sqrt(dim(Boston)[1])
se.hat
```
**(c)**
```{r}
set.seed(1)
boot.fn <- function(data, index) {
    mu <- mean(data[index])
    return (mu)
}
boot(medv, boot.fn, 1000)
```
**(d)**
```{r}
t.test(medv)
```
**(e)**
```{r}
med.hat <- median(medv)
med.hat
```
**(f)**
```{r}
boot.fn <- function(data, index) {
    mu <- median(data[index])
    return (mu)
}
boot(medv, boot.fn, 1000)
```
We get an estimated median value of 21.2 which is equal to the value got in (e), 
with a standard error of 0.3874 which is relatively small compared to median value.

**(g)**  
```{r}
percent.hat <- quantile(medv, c(0.1))
percent.hat
```
**(h)**
```{r}
boot.fn <- function(data, index) {
    mu <- quantile(data[index], c(0.1))
    return (mu)
}
boot(medv, boot.fn, 1000)
```
We get an estimated tenth percentile value of 12.75 which is again equal to the 
value obtained in (g), with a standard error of 0.5113 which is relatively small 
compared to percentile value.

## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

To show that least squares and maximum likelihood estimation (MLE) are equivalent,
consider the linear model:

$$
Y = X\beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I).
$$

The likelihood function for \( Y \) is:

$$
L(\beta, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left( -\frac{1}{2\sigma^2} \| Y - X\beta \|^2 \right).
$$

Taking the log:

$$
\log L(\beta, \sigma^2) = -\frac{n}{2} \log (2\pi\sigma^2) - \frac{1}{2\sigma^2} \| Y - X\beta \|^2.
$$

Maximizing \( L \) with respect to \( \beta \) is equivalent to minimizing:

$$
\| Y - X\beta \|^2.
$$

which is exactly the least squares objective function. Therefore, 
the OLS estimator is the same as the MLE estimator:

$$
\hat{\beta}_{\text{MLE}} = \hat{\beta}_{\text{OLS}} = (X^T X)^{-1} X^T Y.
$$

Thus, we have shown that least squares and MLE are the same in the Gaussian setting.

---

Mallows' \( C_p \) is defined as:

$$
C_p = \frac{1}{n} (RSS + 2d\hat{\sigma}^2),
$$
- \( RSS \) is the **residual sum of squares**,  
- \( d \) is the **number of parameters**,  
- \( \hat{\sigma}^2 \) is an estimate of the error variance.

The AIC is:

$$
AIC = -2 \log L + 2d.
$$

Since:

$$
-2 \log L \approx n \log (RSS/n) + \text{constant},
$$

it follows that:

$$
AIC \approx C_p.
$$

Thus, in the Gaussian setting, \( C_p \) and AIC are equivalent in model selection.

## ISL Exercise 6.6.1 (10pts)
**(a)**  
When performing best subset selection, the model with $k$ predictors is chosen 
from all $C_p^k$ possible models with $k$ predictors, selecting the one with the 
**smallest residual sum of squares (RSS)**.

In **forward stepwise selection**, the model with $k$ predictors is selected 
from the $p - k + 1$ models that result from adding one predictor to the best 
$\mathcal{M}_{k - 1}$-predictor model.

In **backward stepwise selection**, the model with $k$ predictors is selected 
from the $k + 1$ models that result from removing one predictor from the best 
$\mathcal{M}_{k + 1}$-predictor model.

Since best subset selection considers all possible models at each step, 
it always finds the model with the **lowest training RSS**, 
making it the most optimal approach in terms of training error.  

**(b)**  
The model selected by best subset selection has the smallest training RSS 
because it evaluates all possible models with k predictors and chooses the one 
with the lowest residual sum of squares (RSS). In contrast, forward stepwise 
selection and backward stepwise selection only explore a subset of possible 
models, meaning they may not always find the model with the lowest RSS. 
However, in some cases, all three methods might end up selecting the same model.  

**(c) True or False:**  
- ***i*** True.  
- ***ii*** True.  
- ***iii*** False.  
- ***iv*** False.  
- ***v*** False.  

## ISL Exercise 6.6.3 (10pts)  
**(a)**  

> **Part iv - The training RSS steadily decreases as \( s \) increases.**  

LASSO regression constrains the sum of absolute values of the coefficients: 
$\sum_{j=1}^{p} |\beta_j| \leq s$ where $s$ controls the degree of regularization.

As $s$ increases, the constraint loosens, allowing the coefficients $\beta_j$ to 
move closer to their least squares estimates. This increases model flexibility 
and leads to a reduction in training RSS.

When $s$ becomes sufficiently large, the constraint no longer affects the 
solution, meaning the estimated coefficients minimize: 
$RSS = \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2$ 
and match the ordinary least squares (OLS) estimates. Up to this point, 
the training RSS decreases monotonically as $s$ increases.

**(b)**  

> **Part ii - Decrease initially, then eventually increase in a U shape.**

When $s = 0$, the only $\hat{\beta}$ that satisfies $\sum_{j=1}^{p} |\beta_j| \leq s$ 
is the zero vector, meaning the model simply predicts the mean $\hat{y} = \bar{y}$, leading to a very high test RSS.

As $s$ increases, the restriction loosens, allowing the coefficients $\beta_j$ to take on nonzero values. This increases the model‚Äôs flexibility, enabling it to fit the data better and initially decreasing the test RSS.

However, as $s$ continues to increase, the model becomes overly complex, fitting noise in the training data and leading to overfitting. At this stage, test RSS starts rising again, forming a characteristic U-shaped pattern.

**(c)**  

> **Part ii - Steadily increase.**  

As $s$ increases from zero, the constraint region expands, effectively reducing 
$\lambda$ (shrinkage). This increases model flexibility, leading to a steady 
rise in variance. If $s$ becomes large enough that $\hat{\beta}$ falls within 
the unconstrained region, variance stabilizes, as the selected $\hat{\beta}$ 
aligns with the least squares estimate.

**(d)**  

> **Part iv - Steadily Decrease.**  

Similar to part (c), increasing model flexibility reduces bias. As $s$ gets 
larger, constraints loosen, coefficients adjust, and bias is reduced. But as 
soon as the least squares solution lies inside the constraint region, 
bias reduction ceases.  

**(e)**  

> **Part v. - Remain Constant.**  

The irreducible error arises from inherent uncertainty or noise in the system. 
It is invariant to model flexibility since there are some explanatory
variables may be unmeasured or certain variations in $y$ cannot be measured
by $X$. Thus, no matter how well the model is specified, the irreducible error 
is entirely independent of $s$.

$$
\text{Irreducible Error} = \mathbb{E}[(y - f(X))^2]
$$

## ISL Exercise 6.6.4 (10pts)

## ISL Exercise 6.6.5 (10pts)

## ISL Exercise 6.6.11 (30pts)

You must follow the [typical machine learning paradigm](https://ucla-biostat-212a.github.io/2024winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | | | |
| Ridge | | | |
| Lasso | | | |
| ... | | | |

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$